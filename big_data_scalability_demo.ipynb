{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "374ad7cf",
   "metadata": {},
   "source": [
    "# Big Data Scalability Demo: Dask & PySpark\n",
    "\n",
    "This notebook demonstrates scalable analytics using Dask and PySpark on a synthetic clickstream dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc29f760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook contains Dask & Spark pipelines; run locally with required packages.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Synthetic dataset generator\n",
    "N = 10**6  # adjustable size\n",
    "np.random.seed(42)\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'user_id': np.random.randint(1, 10000, N),\n",
    "    'session_id': np.random.randint(1, 100000, N),\n",
    "    'clicks': np.random.poisson(3, N),\n",
    "    'timestamp': pd.date_range('2021-01-01', periods=N, freq='s')\n",
    "})\n",
    "\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3e1d1b-d709-4039-9edd-41cddf79de70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Parquet files found. Generate data with the notebook first.\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "result_pandas = data.groupby('user_id')['clicks'].sum().reset_index()\n",
    "end = time.time()\n",
    "print(f'Pandas processing time: {end - start:.2f} seconds')\n",
    "result_pandas.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1926d43-b254-46f0-b12b-b6fc2b0ef09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import dask.dataframe as dd\n",
    "    \n",
    "    ddf = dd.from_pandas(data, npartitions=4)\n",
    "    start = time.time()\n",
    "    result_dask = ddf.groupby('user_id')['clicks'].sum().compute()\n",
    "    end = time.time()\n",
    "    print(f'Dask processing time: {end - start:.2f} seconds')\n",
    "    result_dask.head()\n",
    "except ImportError:\n",
    "    print('⚠️ Dask not installed. Please install with: pip install dask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f8f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "    spark = SparkSession.builder.master('local[*]').appName('BigDataDemo').getOrCreate()\n",
    "    sdf = spark.createDataFrame(data)\n",
    "\n",
    "    start = time.time()\n",
    "    result_spark = sdf.groupBy('user_id').agg(spark_sum('clicks').alias('total_clicks'))\n",
    "    result_spark.show(5)\n",
    "    end = time.time()\n",
    "    print(f'PySpark processing time: {end - start:.2f} seconds')\n",
    "\n",
    "    spark.stop()\n",
    "except ImportError:\n",
    "    print('⚠️ PySpark not installed. Please install with: pip install pyspark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f894e18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
